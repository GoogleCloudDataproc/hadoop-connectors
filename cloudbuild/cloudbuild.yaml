# Copyright 2019 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#      http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

steps:
  # 1. Create a Docker image containing hadoop-connectors repo
  - name: 'gcr.io/cloud-builders/docker'
    id: 'docker-build'
    args: ['build', '--tag=gcr.io/$PROJECT_ID/dataproc-hadoop-connectors-presubmit', '-f', 'cloudbuild/Dockerfile', '.']

  # 2. Run unit tests
  - name: 'gcr.io/$PROJECT_ID/dataproc-hadoop-connectors-presubmit'
    id: 'unit-tests'
    waitFor: ['docker-build']
    entrypoint: 'bash'
    args: ['/hadoop-connectors/cloudbuild/presubmit.sh', 'unittest']
    env:
      - 'CODECOV_TOKEN=$_CODECOV_TOKEN'
      - 'VCS_BRANCH_NAME=$BRANCH_NAME'
      - 'VCS_COMMIT_ID=$COMMIT_SHA'
      - 'VCS_TAG=$TAG_NAME'
      - 'CI_BUILD_ID=$BUILD_ID'

  # 3. Create a zonal bucket for integration tests
  - name: 'gcr.io/cloud-builders/gcloud'
    id: 'create-zonal-bucket'
    waitFor: ['docker-build']
    entrypoint: 'bash'
    args:
      - -c
      - |
        # Generate a unique name for the zonal bucket. $SHORT_SHA is a valid Cloud Build substitution.
        BUCKET_NAME="zonal-bucket-$SHORT_SHA"
        
        # Fetch the zone of the running worker from the metadata server. Escape shell commands with $$.
        WORKER_ZONE_FULL_PATH=$$(curl -s "http://metadata.google.internal/computeMetadata/v1/instance/zone" -H "Metadata-Flavor: Google")
        WORKER_ZONE=$$(echo "$$WORKER_ZONE_FULL_PATH" | awk -F/ '{print $$NF}')
        
        # Extract the region from the zone. Escape shell variables with $$.
        REGION=$$(echo "$$WORKER_ZONE" | cut -d'-' -f1,2)
        
        # Create the zonal bucket with the specified placement and properties
        gcloud storage buckets create "gs://$$BUCKET_NAME" \
          --location="$$REGION" \
          --placement="$$WORKER_ZONE" \
          --default-storage-class=RAPID \
          --enable-hierarchical-namespace \
          --uniform-bucket-level-access
        
        # Persist the bucket name for subsequent steps by writing it to a file in the workspace
        echo "$$BUCKET_NAME" > /workspace/zonal_bucket_name.txt

  # 4. Run integration tests using the zonal bucket
  - name: 'gcr.io/$PROJECT_ID/dataproc-hadoop-connectors-presubmit'
    id: 'integration-tests'
    waitFor: ['create-zonal-bucket']
    entrypoint: 'bash'
    args:
      - -c
      - |
        # Read the bucket name from the file and export it as an environment variable. Escape shell command with $$.
        export GCS_ZONAL_TEST_BUCKET=$$(cat /workspace/zonal_bucket_name.txt)
        echo "Using zonal bucket '$$GCS_ZONAL_TEST_BUCKET' for integration tests."
        
        # Execute the integration test script
        /hadoop-connectors/cloudbuild/presubmit.sh integrationtest
    env:
      - 'GCS_TEST_PROJECT_ID=$PROJECT_ID'
      - 'CODECOV_TOKEN=$_CODECOV_TOKEN'
      - 'VCS_BRANCH_NAME=$BRANCH_NAME'
      - 'VCS_COMMIT_ID=$COMMIT_SHA'
      - 'VCS_TAG=$TAG_NAME'
      - 'CI_BUILD_ID=$BUILD_ID'
      - 'GCS_TEST_DIRECT_PATH_PREFERRED=false'
#
#  # 5. Clean up and delete the zonal bucket
#  - name: 'gcr.io/$PROJECT_ID/dataproc-hadoop-connectors-presubmit'
#    id: 'delete-zonal-bucket'
#    waitFor: ['integration-tests']
#    entrypoint: 'bash'
#    args:
#      - -c
#      - |
#        # Check if the bucket name file exists before attempting deletion
#        if [ -f /workspace/zonal_bucket_name.txt ]; then
#          # Escape shell command with $$
#          BUCKET_NAME=$$(cat /workspace/zonal_bucket_name.txt)
#          echo "Cleaning up and deleting bucket: gs://$$BUCKET_NAME"
#          gcloud storage rm -r "gs://$$BUCKET_NAME"
#        else
#          echo "Bucket name file not found. Skipping cleanup."
#        fi

# Tests take on average 25 minutes to run
timeout: 2400s

options:
  pool:
    name: 'projects/cloud-dataproc-ci/locations/us-central1/workerPools/integ-test-connector-pool'
## Copyright 2019 Google LLC
##
## Licensed under the Apache License, Version 2.0 (the "License");
## you may not use this file except in compliance with the License.
## You may obtain a copy of the License at
##
##      http://www.apache.org/licenses/LICENSE-2.0
##
## Unless required by applicable law or agreed to in writing, software
## distributed under the License is distributed on an "AS IS" BASIS,
## WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
## See the License for the specific language governing permissions and
## limitations under the License.
#
#steps:
#  # 1. Create a Docker image containing hadoop-connectors repo
#  - name: 'gcr.io/cloud-builders/docker'
#    id: 'docker-build'
#    args: ['build', '--tag=gcr.io/$PROJECT_ID/dataproc-hadoop-connectors-presubmit', '-f', 'cloudbuild/Dockerfile', '.']
#
#  # 2. Run unit tests
#  - name: 'gcr.io/$PROJECT_ID/dataproc-hadoop-connectors-presubmit'
#    id: 'unit-tests'
#    waitFor: ['docker-build']
#    entrypoint: 'bash'
#    args: ['/hadoop-connectors/cloudbuild/presubmit.sh', 'unittest']
#    env:
#      - 'CODECOV_TOKEN=$_CODECOV_TOKEN'
#      - 'VCS_BRANCH_NAME=$BRANCH_NAME'
#      - 'VCS_COMMIT_ID=$COMMIT_SHA'
#      - 'VCS_TAG=$TAG_NAME'
#      - 'CI_BUILD_ID=$BUILD_ID'
#
#  # 3. Run integration tests
#  - name: 'gcr.io/$PROJECT_ID/dataproc-hadoop-connectors-presubmit'
#    id: 'integration-tests'
#    waitFor: ['docker-build']
#    entrypoint: 'bash'
#    args: ['/hadoop-connectors/cloudbuild/presubmit.sh', 'integrationtest']
#    env:
#      - 'GCS_TEST_PROJECT_ID=$PROJECT_ID'
#      - 'CODECOV_TOKEN=$_CODECOV_TOKEN'
#      - 'VCS_BRANCH_NAME=$BRANCH_NAME'
#      - 'VCS_COMMIT_ID=$COMMIT_SHA'
#      - 'VCS_TAG=$TAG_NAME'
#      - 'CI_BUILD_ID=$BUILD_ID'
#      - 'GCS_TEST_DIRECT_PATH_PREFERRED=false'
#
## Tests take on average 25 minutes to run
#timeout: 2400s
#
#options:
#  pool:
#    name: 'projects/cloud-dataproc-ci/locations/us-central1/workerPools/integ-test-connector-pool'

